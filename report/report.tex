\documentclass{article}
\usepackage{amsmath}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{%
	breaklinks=true,
	colorlinks,
}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{siunitx}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\smax}{smax}

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

\lstset{%
	language=Python,
	basicstyle=\ttm,
	morekeywords={self},              % Add keywords here
	keywordstyle=\ttb\color{deepblue},
	emph={MyClass,__init__},          % Custom highlighting
	emphstyle=\ttb\color{deepred},    % Custom highlighting style
	stringstyle=\color{deepgreen},
	frame=tb,                         % Any extra options here
	showstringspaces=false
}

\newcommand{\myparagraph}[1]{\noindent\textbf{#1 ---}}

\title{Project A \\ECE1512 Winter 2021}
\author{Brendan Duke\\Student ID: dukebren\\Date due: Mar. 1, 2021\\Date handed in: Mar. 1, 2021}
\date{}


\begin{document}

\maketitle
\clearpage


\section{Part 1: 1D Digit Classification}


\subsection{Task 1: 1D Digit Classification}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-class-accuracy}
	\caption{\label{fig:mnist1d-classwise-accuracy}Class-wise accuracy on MNIST-1D\@.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-roc-curve}
	\caption{\label{fig:mnist1d-roc-curve}ROC-AUC curves on MNIST-1D\@.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-confusion-matrix}
	\caption{\label{fig:mnist1d-confusion-matrix}MNIST-1D confusion matrix.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-precision-recall}
	\caption{\label{fig:mnist1d-precision-recall}MNIST-1D precision-recall curves.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-failures}
	\caption{\label{fig:mnist1d-failures}MNIST-1D misclassification examples.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/mnist1d-successes}
	\caption{\label{fig:mnist1d-successes}MNIST-1D correct classification.}
\end{figure}

The overall (example-wise) classification accuracy on the test set was~\num{0.877}, computed with the code at \url{https://github.com/dukebw/interpretable}.
The class-wise accuracy on MNIST-1D is given in Fig~\ref{fig:mnist1d-classwise-accuracy}.
The ROC-AUC curves are in Fig~\ref{fig:mnist1d-roc-curve}.
The confusion matrix is in Fig~\ref{fig:mnist1d-confusion-matrix}.
Fig~\ref{fig:mnist1d-precision-recall} contains precision-recall curves, F1 scores, and average precision (AP) on the test set.

Fig~\ref{fig:mnist1d-failures} and Fig~\ref{fig:mnist1d-successes} show qualitative examples of misclassifications and correct classifications, respectively.
Referring back to the confusion matrix (Fig~\ref{fig:mnist1d-confusion-matrix}), the two most often misclassified classes were digits~\num{4} and~\num{9}, while~\num{9} was also frequently confused with~\num{5} and~\num{8}.
Digits~\num{9} and~\num{4} are similar in appearance even in 2D handwriting.
MNIST-1D introduces noise and random transformations such as shear, padding, and rotation to each digit.
As seen in Fig~\ref{fig:mnist1d-failures}, this noise often makes the digits indistinguishable to even the human eye.
Indistinguishability to even the human eye indicates unavoidable error (Bayes error).
Therefore the high misclassification rate for digit~\num{9} can be attributed to the Bayes error rate in MNIST-1D, caused by the injection of random noise.


\subsection{Task 2: CNN Interpretation}


I chose to implement ``Understanding Deep Networks via Extremal Perturbations and Smooth Masks''~\cite{fong2019understanding}, which I will refer to as Extremal Perturbations (EP).

\myparagraph{Research gap} EP fills a research gap left by existing methods at its time of publication.
Firstly, existing methods had been found to produce the same attribution no matter which output neuron was analysed~\cite{mahendran2016salient}.
In some cases even model weights were ignored in the attribution~\cite{adebayo2018sanity}.
In contrast, experiments where model weights were progressively randomized demonstrated that EP is sensitive to changes in model weights.
Furthermore, EP can be extended to find which channels are salient, showing that EP can differentiate between the output of different neurons.
Another research gap filled by EP is left by prior work~\cite{fong2017interpretable} that required balancing several energy terms, each with its own coefficient.
These coefficients are hyperparameters, and the output attribution depends on the particular setting of the hyperparameters.
EP contributes a method that fills this need for hyperparameter-free attributions.
EP produces the same attribution regardless of hyperparameter, because EP solves an optimization problem that is independent of hyperparameter setting.
EP improved the existing literature as a method that is both sensitive to model weights and distinguishes between output neurons, as well as produces a concrete attribution that is independent of hyperparameters.


\myparagraph{Novelty / contribution} The contributions of EP are twofold.
Firstly, the method of extremal perturbations is introduced.
An extremal perturbation produces the maximum network activation for a fixed perturbation area.
Previous methods balanced energy terms in their objective, and hence extremal perturbations is novel.
Secondly, the extremal perturbation framework is extended to work on intermediate activations over channels, instead of just the input.
In this way, attribution goes beyond input spatial attribution and can be done over channels and intermediate layers.


\myparagraph{Methodologies} Here I will follow the Extremal Perturbations notation to describe the method.
The fundamental idea of EP is to find a mask~$m_a$ for a fixed area budget~$a$ that represents the greatest attribution of the inputs for a given output neuron.
This is done by solving a two-level optimization problem: on one level a mask~$m_a$ is found that maximizes the activations for a fixed area budget~$a$, and on the other level the area budget is minimized for a fixed activation threshold.
On the first level, suppose that the area budget~$a$ is fixed for a given input~$x$ with the neural network model represented by~$\Phi$.
The activation-maximizing mask~$m_a$ is then
\begin{equation}
	m_a = \argmax_{m: {\lVert m\rVert}_1 = a, m \in \mathcal{M}} \Phi(m\otimes x),
	\label{eq:mask-maximal-output}
\end{equation}
where~$m\in\mathcal{M}$ indicates that the masks~$m$ are constrained to a smooth manifold~$\mathcal{M}$
This output-maximizing mask (\ref{eq:mask-maximal-output}) depends on the area~$a$, so the introduction of the second level of optimization removes this dependency.
Given a desired lower bound~$\Phi_0$ on the outputs, the second level finds a minimal area~$a^*$ as
\begin{equation}
	a^* = \min\{a\,:\,\Phi(m_a\otimes x)\geq\Phi_0\}.
	\label{eq:area-minimal}
\end{equation}
The mask~$m_{a^*}$ resulting from alternating between both levels of optimization is called extremal, because there is no smaller mask that produces output greater than the lower bound~$\Phi_0$.
Hence, mask~$m_{a^*}$ concretely represents the attribution since not only does~$m_{a^*}$ produce a large response from the neural network~$\Phi$, there is also no smaller (sufficiently smooth) area of the input that can produce an equally large response.


\myparagraph{Advantages / disadvantages} The advantages of EP are the independence of EP's attribution from choice of hyperparameters, and EP's sensitivity to both model weights and output neuron.
One disadvantage is that optimization EP is difficult as posed, since the output-maximizing mask (\ref{eq:mask-maximal-output}) has gradients that are zero almost everywhere.
EP fixes this by introducing a ``smooth max'' operator~$\smax$,
\begin{equation}
	\smax_{u\in\Omega; T} f(u) = \frac{\sum_{u\in\Omega}f(u)\exp f(u)/T}{\sum_{u\in\Omega}\exp f(u)/T}
	\label{eq:smax}
\end{equation}
for image spatial coordinates~$u$, image space~$\Omega$, and temperature~$T$.
However, this smooth max operator introduces the temperature~$T$ as a hyperparameter.
So, to retain the ``hyperparameter independence'' advantage, EP's attributions must prove to be independent of temperature~$T$.


\myparagraph{Still interpretable in difficult scenarios?} Based on the output-maximizing mask formulation (\ref{eq:mask-maximal-output}), EP should handle difficult scenarios except when the smoothness assumption is broken.
If the smoothness constraint~$m\in\mathcal{M}$ does not hold true, then EP will produce an incorrect attribution.
EP defines smooth masks by convolving a prototype or ``auxiliary'' mask with a Gaussian kernel.
Therefore, if the model's attribution contained high spatial frequencies, those frequencies would be removed by the smoothing.
In this difficult scenario of high frequency true attribution, EP would no longer be interpretable.


\myparagraph{Can it analyze and inspect the cases of misclassification? Why?} EP should in theory be able to analyze and inspect the cases of misclassification shown in Fig~\ref{fig:mnist1d-failures}.
EP identifies the most salient region of an input using a bilevel optimization, which alternates between maximizing outputs for a fixed area budget, and minimizing area for a fixed output lower bound.
This would be especially helpful in identifying salient regions of the MNIST-1D digits, since these digits are mostly noise introduced in dataset creation (Fig~\ref{fig:mnist1d-failures}).


\begin{figure}[ht]
	\includegraphics[width=\textwidth]{images/mnist1d-ep-successes}
	\caption{\label{fig:mnist1d-ep-successes}MNIST-1D Extremal Perturbation visualization for correct classification.
		Top row: raw input digit.
		Middle row: perturbed input where unimportant regions have been masked (blurred) out.
		Attribution is determined by comparing the perturbed (second row) to the input (first row).
		The model's prediction is attributable to the first row regions preserved in the second row.
		Bottom row: template of the label, for reference.}
\end{figure}

\begin{figure}[ht]
	\includegraphics[width=\textwidth]{images/mnist1d-ep-failures}
	\caption{\label{fig:mnist1d-ep-failures}MNIST-1D Extremal Perturbation visualization for misclassification.
		Top row: raw input digit.
		Second row: perturbed input where unimportant regions have been blurred out.
		Third row: template of the label.
		Fourth row: template of the predicted class.}
\end{figure}

\myparagraph{Implementation (2a)} I implemented Extremal Perturbation (EP) in TensorFlow for MNIST-1D\@.
1D EP takes the digit represented as a vector of ``height'' ($y$ axis) values as input, and outputs a 1D mask of floating point values between zero and one.
This 1D mask is the explanation map that attributes the model's prediction to important input features.
The 1D mask is near one for regions of the~$x$ axis that have important input features, and near zero for unimportant regions.
The 1D mask is used to blur out the unimportant regions (Figs~\ref{fig:mnist1d-ep-successes} \& \ref{fig:mnist1d-ep-failures}).


\myparagraph{Qualitative results} 1D EP produces qualitatively meaningful attributions for examples where the model was correctly classified (Fig~\ref{fig:mnist1d-ep-successes}).
Fig~\ref{fig:mnist1d-ep-successes} shows the result from running EP on a set of correctly classified test inputs.
Attribution is determined in Fig~\ref{fig:mnist1d-ep-successes} by comparing the first and second rows.
EP works by blurring out unimportant regions, so the region of the first row that is left over in the second row is the input region to which EP attributes the model's prediction.
For instance, in Fig~\ref{fig:mnist1d-ep-successes} column 2, the large ``U'' shaped region is leftover, and matches the template for the label (column 2, bottom).
The other regions of column 2 have been deemed unimportant by EP, and therefore EP blurred them out in the perturbation step ($m\otimes x$ in Eq~\ref{eq:mask-maximal-output}).
In many cases the highlighted region is similar to the corresponding template for the label (compare second and third rows in columns 2 to 8).
However in some cases (columns 1, 9, and 10) the relation between the attributed region and the template is less obvious.
This could be due to the size of the region chosen (30\% of the input), coupled with difficulty interpreting MNIST-1D (note the high human error rate indicating high Bayes error rate).

EP also explains mispredictions (Fig~\ref{fig:mnist1d-ep-failures}) well.
For these misclassifications, in some cases (columns 6 and 7-10) the templates are similar to begin with, as seen by comparing the label template with the predicted class template (Fig~\ref{fig:mnist1d-ep-failures} rows 3 and 4).
For other cases, EP highlights the reason for the model's confusion.
For example, in column 7 the model has incorrectly ignored the leftmost region (corresponding to the label 3), and instead focused on a region on the right, which the model interprets as digit zero.
The region on the right is spurious noise introduced by the random transformations used to create the dataset, but the model interprets that noise as the ``U'' shape (digit zero).
Overall, by working in the input space EP produced interpretable attributions with little tuning --- the main hyperparameter I had to change was the Gaussian standard deviation (used to create the smooth mask manifold), which I made smaller due to the smaller spatial resolution of MNIST-1D compared to ImageNet.


\section{Part 2: Histopathological Tissue Classification}

\subsection{Task 3: Biomedical Image Classification and Interpretation}

\myparagraph{Evaluation metrics on HMT} I evaluated per-class accuracy (Fig~\ref{fig:hmt-classwise-accuracy}), ROC-AUC curves (Fig~\ref{fig:hmt-roc-curve}), confusion matrix (Fig~\ref{fig:hmt-confusion-matrix}), and precision-recall curves (Fig~\ref{fig:hmt-precision-recall}) on HMT\@.
\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-class-accuracy}
	\caption{\label{fig:hmt-classwise-accuracy}Class-wise accuracy on HMT\@.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-roc-curve}
	\caption{\label{fig:hmt-roc-curve}ROC-AUC curves on HMT\@.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-confusion-matrix}
	\caption{\label{fig:hmt-confusion-matrix}HMT confusion matrix.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-precision-recall}
	\caption{\label{fig:hmt-precision-recall}HMT precision-recall curves.}
\end{figure}

\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-ep-successes}
	\caption{\label{fig:hmt-ep-successes}HMT Extremal Perturbation visualization for correct classification.
		Top row: input images.
		Bottom row: perturbed input where unimportant regions have been blurred out.
		Compare bottom and top row images to determine attribution.}
\end{figure}


\begin{figure}[t]
	\includegraphics[width=\textwidth]{images/hmt-ep-failures}
	\caption{\label{fig:hmt-ep-failures}HMT Extremal Perturbation visualization for correct classification.
		Top row: input images.
		Bottom row: perturbed input where unimportant regions have been blurred out.
		Compare bottom and top row images to determine attribution.}
\end{figure}

\myparagraph{Extremal Perturbation Implementation for HMT} As for MNIST-1D, I implemented Extremal Perturbation in TensorFlow for attributing model predictions on HMT images.
Example attributions for successful classifications are in Fig~\ref{fig:hmt-ep-successes}.
Example attributions for misclassifications are in Fig~\ref{fig:hmt-ep-failures}.


\subsection{Task 4: Quantitative Evaluation of the Attribution Methods}


\myparagraph{Drop \% and Increase \%} As a perturbation method Extremal Perturbation (EP) is ideally suited for evaluating ``Drop \%'' and ``Increase \%''.
Perturbation methods work by selectively deleting and preserving parts of the input and observing the effect on the model's output activations.
For example, EP blurs out (deletes) regions of histopathology images that are not important for model predictions, leaving high frequency detail only in salient regions (Figs~\ref{fig:hmt-ep-successes} \& \ref{fig:hmt-ep-failures}).
Since EP works by perturbations, EP is immediately interpretable in terms of Drop \% and Increase \%.
In order to evaluate Drop \% and Increase \%, it is sufficient to compare the confidence of the model on the raw (unperturbed) input against the final perturbed input.
Because the rest of the image has been blurred out, in EP the final perturbed input essentially contains only the salient region, making the Drop \% and Increase \% calculation intuitive.


\myparagraph{Qualitative and quantitative results discussion}


\clearpage
\appendix
\section{Appendix: Program Listings}

The code is available at \url{https://github.com/dukebw/interpretable}.
Note that for the implementation I referred to \hyperlink{https://github.com/facebookresearch/TorchRay}{TorchRay}'s extremal perturbations.
But, the conversion to 1D for MNIST-1D, and from PyTorch to TensorFlow for both MNIST-1D and HMT, was non-trivial and required significant effort.

\small
\bibliographystyle{ieee}
\bibliography{report}

\end{document}
